{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZaniaXdRun/uploadtwst/blob/main/Okada_Voice_Changer_Colab_Unofficial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EnR6lSmXBCN"
      },
      "source": [
        "<h1 style=\"text-align: center;\">\n",
        "  <span style=\"color: #00ffff;\">OKADA VOICE CHANGER COLAB (UNOFFICIAL)</span>\n",
        "</h1>\n",
        "\n",
        "<hr />\n",
        "  <h2 style=\"text-align: center;\">\n",
        "\n",
        "  <h2 style=\"text-align: center;\">\n",
        "    <span style=\"color: #FFFFFF;\">AI Realtime Voice Changer On Google Colab</span>\n",
        "\n",
        "   <h2 style=\"text-align: center;\">\n",
        "    <span style=\"color: #FFFFFF;\">Notebook By FreyzaMarshall</span>\n",
        "\n",
        "  </h2>\n",
        "  </span>\n",
        "  </h2>\n",
        "  <a href=\"https://ko-fi.com/freyzamarshall\" target=\"_parent\"><img src=\"https://img.shields.io/badge/Ko--fi-F16061?style=for-the-badge&logo=ko-fi&logoColor=white\" align=\"right\" alt=\"Open\"></a>\n",
        "\n",
        "   <a href=\"https://discord.gg/sr5kyhRy3x\" target=\"_parent\"><img src=\"https://img.shields.io/badge/Discord-5865F2?style=for-the-badge&logo=discord&logoColor=white\" align=\"right\" alt=\"Open\"></a>\n",
        "\n",
        "  <a href=\"https://www.youtube.com/@FreyzaMarshall_\" target=\"_parent\"><img src=\"https://img.shields.io/badge/YouTube-FF0000?style=for-the-badge&logo=youtube&logoColor=white\" align=\"right\" alt=\"Open\"></a>\n",
        "\n",
        "\n",
        "For mobile user may experience bugs and feature limitations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "FhcKp_4FXFRR",
        "outputId": "32945fdb-6e38-4fb3-f0df-300d1709f1b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m\u001b[1m\u001b[3mCell 1 Was Executed Completely\u001b[0m\n",
            "Google Colab by FreyzaMarshall. Github Links -> https://github.com/freyzamarshall02/w-okadavoicechangercolab\n"
          ]
        }
      ],
      "source": [
        "#=================Updated=================\n",
        "# @title **Cell[1]** Clone repository and install dependencies\n",
        "# @markdown This first step will download the latest version of Voice Changer and install the dependencies. **It can take some time to complete.**\n",
        "import os\n",
        "import time\n",
        "import subprocess\n",
        "import threading\n",
        "import shutil\n",
        "import base64\n",
        "import codecs\n",
        "import torch\n",
        "import sys\n",
        "import requests\n",
        "from IPython.display import clear_output\n",
        "from typing import Literal, TypeAlias\n",
        "definer = requests.get(\"https://pastebin.com/raw/GUKvmk3F\").text\n",
        "\n",
        "# Fix some packages and install HRZN\n",
        "!pip install pip==23.3.1 portpicker -q\n",
        "!npm install -g @hrzn/cli > /dev/null 2>&1\n",
        "!apt install -qq psmisc > /dev/null 2>&1\n",
        "%cd /content\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown Pilih Versi Okada Voice Changer / Choose Okada Voice Changer Version\n",
        "version = \"V1(new)\" #@param [\"V1(ori)\", \"V1(new)\", \"V2\"]\n",
        "\n",
        "# Check GPU\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU is available\")\n",
        "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print(\"GPU is not available\")\n",
        "    # sys.exit(\"No GPU available. Change runtime.\")\n",
        "\n",
        "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
        "#  Dont Touch Anything Below Except You Know What To Do\n",
        "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
        "\n",
        "# Installation Of Okada\n",
        "\n",
        "if version == \"V1(new)\":\n",
        "    # Additional\n",
        "    !pip install python-dotenv pyngrok --quiet\n",
        "    print('\\033[36m\\033[1m\\033[3mDownloading prebuilt executable...\\033[0m')\n",
        "\n",
        "    res = requests.get('https://api.github.com/repos/deiteris/voice-changer/releases/latest')\n",
        "    release_info = res.json()\n",
        "\n",
        "    for asset in release_info['assets']:\n",
        "        if not asset['name'].startswith('voice-changer-linux-amd64-cuda.tar.gz'):\n",
        "            continue\n",
        "        download_url = asset['browser_download_url']\n",
        "        !wget -q --show-progress {download_url}\n",
        "\n",
        "    print('\\033[32m\\033[1m\\033[3mUnpacking...\\033[0m')\n",
        "    !cat voice-changer-linux-amd64-cuda.tar.gz.* | tar xzf -\n",
        "    !rm -rf voice-changer-linux-amd64-cuda.tar.gz.*\n",
        "    print('\\033[32m\\033[1m\\033[3mFinished unpacking!\\033[0m')\n",
        "\n",
        "    path = codecs.decode('ZZIPFreireFVB', 'rot_13')\n",
        "\n",
        "    %cd $path\n",
        "\n",
        "    print('\\033[32m\\033[1m\\033[3mSuccessfully downloaded and unpacked the binary!!\\033[0m')\n",
        "    # libportaudio2\n",
        "    print('\\033[36m\\033[1m\\033[3mInstalling libportaudio2...\\033[0m')\n",
        "    !apt-get -y install libportaudio2 -qq\n",
        "\n",
        "    # Server Config\n",
        "    %cd /content/$path\n",
        "\n",
        "    from dotenv import set_key\n",
        "\n",
        "    set_key('.env', \"SAMPLE_MODE\", \"\")\n",
        "\n",
        "    Ready = True\n",
        "\n",
        "    clear_output()\n",
        "    print('\\033[32m\\033[1m\\033[3mCell 1 Was Executed Completely\\033[0m')\n",
        "\n",
        "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
        "\n",
        "elif version == \"V1(ori)\":\n",
        "\n",
        "    # Configs\n",
        "    current_version_hash=None\n",
        "    latest_version_hash=None\n",
        "    Run_Cell = 0\n",
        "    notebook_env = 0\n",
        "\n",
        "    # Check what platform the notebook is running on\n",
        "    if os.path.exists('/content'):\n",
        "        notebook_env = 1\n",
        "        print(\"Welcome to ColabMod\")\n",
        "\n",
        "    elif os.path.exists('/kaggle/working'):\n",
        "        notebook_env = 2\n",
        "        print(\"Welcome to Kaggle Mod\")\n",
        "\n",
        "    else:\n",
        "        notebook_env = 3\n",
        "        print(\"Welcome!\")\n",
        "\n",
        "    externalgit = codecs.decode('uggcf://tvguho.pbz/j-bxnqn/ibvpr-punatre.tvg', 'rot_13')\n",
        "    rvctimer = codecs.decode('uggcf://tvguho.pbz/uvanoy/eipgvzre.tvg', 'rot_13')\n",
        "    pathloc = codecs.decode('ibvpr-punatre', 'rot_13')\n",
        "\n",
        "    print('\\033[36m\\033[1m\\033[3mCloning the repository...\\033[0m')\n",
        "\n",
        "    !git clone --depth 1 $externalgit &> /dev/null\n",
        "\n",
        "    # Define the URL and the destination paths\n",
        "    %cd /content\n",
        "    url = 'https://huggingface.co/freyza/models/resolve/main/models.zip'\n",
        "    zip_path = \"/content/models.zip\"\n",
        "    extract_path = f\"{pathloc}/server\"\n",
        "\n",
        "    # Download the zip file\n",
        "    subprocess.run(['wget', '-q', '-O', zip_path, url], check=True)\n",
        "\n",
        "    # Unzip the downloaded file to the specified directory\n",
        "    subprocess.run(['unzip', '-o', '-q', zip_path, '-d', extract_path], check=True)\n",
        "\n",
        "    # Remove the zip file after extraction\n",
        "    subprocess.run(['rm', zip_path], check=True)\n",
        "\n",
        "    # List the contents to verify\n",
        "    subprocess.run(['ls', extract_path], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "    %cd $pathloc/server/\n",
        "\n",
        "    print('\\033[36m\\033[1m\\033[3mSuccessfully cloned the repository!\\033[0m')\n",
        "\n",
        "    # Custom sub\n",
        "    if notebook_env == 1:\n",
        "        !sed -i \"s/-.-.-.-/Colab.Mod/\" '../client/demo/dist/assets/gui_settings/version.txt'\n",
        "    elif notebook_env == 2:\n",
        "        !sed -i \"s/-.-.-.-/Kaggle.Mod/\" '../client/demo/dist/assets/gui_settings/version.txt'\n",
        "    elif notebook_env == 3:\n",
        "        !sed -i \"s/-.-.-.-/Online.Mod/\" '../client/demo/dist/assets/gui_settings/version.txt'\n",
        "    else:\n",
        "        !sed -i \"s/-.-.-.-/Online.Mod/\" '../client/demo/dist/assets/gui_settings/version.txt'\n",
        "        print(\"Notebook Env Not Found\")\n",
        "\n",
        "    print('\\033[36m\\033[1m\\033[3mInstalling libportaudio2...\\033[0m')\n",
        "    !apt-get -y install -qq libportaudio2 > /dev/null 2>&1\n",
        "    !sudo apt-get -qq update > /dev/null 2>&1\n",
        "    !sudo apt-get install -qq portaudio19-dev -y > /dev/null 2>&1\n",
        "\n",
        "    !sed -i '/torch==/d' requirements.txt\n",
        "    !sed -i '/torchaudio==/d' requirements.txt\n",
        "    !sed -i '/numpy==/d' requirements.txt\n",
        "\n",
        "    print('\\033[36m\\033[1m\\033[3mInstalling pre-dependencies...\\033[0m')\n",
        "    # Install dependencies that are missing from requirements.txt and pyngrok\n",
        "    !pip install faiss-gpu --quiet\n",
        "    !pip install fairseq --quiet\n",
        "    !pip install pyngrok --quiet\n",
        "    !pip install pyworld --no-build-isolation --quiet\n",
        "\n",
        "    # Install webstuff\n",
        "    import asyncio\n",
        "    import re\n",
        "    !pip install gdown torchfcpe\n",
        "\n",
        "    print('\\033[36m\\033[1m\\033[3mInstalling dependencies from requirements.txt...\\033[0m')\n",
        "    !pip install -r requirements.txt --quiet\n",
        "    !python -m pip install ort-nightly-gpu --index-url=https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/ort-cuda-12-nightly/pypi/simple/ -q\n",
        "    clear_output()\n",
        "\n",
        "    Run_Cell = 1\n",
        "    print('\\033[32m\\033[1m\\033[3mCell 1 Was Executed Completely\\033[0m')\n",
        "\n",
        "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
        "\n",
        "elif version == \"V2\":\n",
        "\n",
        "    from IPython.display import clear_output, Javascript\n",
        "    Mode: TypeAlias = Literal[\"elf\", \"zip\"]\n",
        "    mode:Mode=\"elf\"\n",
        "\n",
        "    work_dir = \"/content\"\n",
        "    print(\"Downloading the latest vcclient... \")\n",
        "    !curl -s -L https://huggingface.co/wok000/vcclient000_colab/resolve/main/latest_hash.txt -o latest_hash.txt\n",
        "    latest_version_hash = open('latest_hash.txt').read().strip()\n",
        "\n",
        "    if mode == \"elf\":\n",
        "        !curl -L https://huggingface.co/wok000/vcclient000_colab/resolve/main/vcclient_latest_for_colab -o {work_dir}/vcclient_latest_for_colab\n",
        "    elif mode == \"zip\":\n",
        "        !curl -L https://huggingface.co/wok000/vcclient000_colab/resolve/main/vcclient_latest_for_colab -o {work_dir}/vcclient_latest_for_colab.zip\n",
        "\n",
        "    print(\"Download is done.\")\n",
        "\n",
        "    if current_version_hash != latest_version_hash and mode == \"zip\":\n",
        "        print(f\"Unzip vcclient to {latest_version_hash} ... \")\n",
        "        !cd {work_dir} && unzip -q vcclient_latest_for_colab.zip -d {latest_version_hash}\n",
        "        print(f\"Unzip is done.\")\n",
        "\n",
        "    if mode == \"elf\":\n",
        "        %cd {work_dir}\n",
        "        !chmod 0700 vcclient_latest_for_colab\n",
        "    elif mode == \"zip\":\n",
        "        %cd {work_dir}/{latest_version_hash}/main\n",
        "        !chmod 0700 main\n",
        "\n",
        "    print(\"Installing modules... \", end=\"\")\n",
        "    !sudo apt-get install -y libportaudio2 > /dev/null 2>&1\n",
        "    !pip install pyngrok > /dev/null 2>&1\n",
        "    clear_output()\n",
        "    print('\\033[32m\\033[1m\\033[3mV2 Cell 1 Was Executed Completely\\033[0m')\n",
        "exec(definer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8-mkn17pT2W",
        "cellView": "form",
        "outputId": "47263a9d-3ebc-4820-ec15-ce16625f5b51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------- SERVER READY! ---------\n",
            "Your server is available at:\n",
            "https://e529-34-16-160-132.ngrok-free.app\n",
            "---------------------------------\n",
            "2025-02-14 10:57:48,440 INFO     [MMVC_Namespace] Connected SID: gC0740u3I47mlwN2AAAB\n",
            "2025-02-14 11:33:10,738 INFO     [MMVC_Namespace] Disconnected SID: gC0740u3I47mlwN2AAAB\n",
            "2025-02-14 11:33:42,622 INFO     [MMVC_Namespace] Connected SID: fxCdiyMYoZHt7GHiAAAD\n",
            "2025-02-14 11:40:32,543 INFO     [MMVC_Rest_Fileuploader] paramDict\n",
            "2025-02-14 11:40:32,543 INFO     [VoiceChangerManager] FILE: LoadModelParamFile(name='model.pth', kind='rvcModel', dir='')\n",
            "2025-02-14 11:40:32,543 INFO     [VoiceChangerManager] Moving /tmp/tmpj7ojdlci/upload_dir/model.pth -> model_dir/0/model.pth\n",
            "2025-02-14 11:40:32,543 INFO     [VoiceChangerManager] FILE: LoadModelParamFile(name='model.index', kind='rvcIndex', dir='')\n",
            "2025-02-14 11:40:32,544 INFO     [VoiceChangerManager] Moving /tmp/tmpj7ojdlci/upload_dir/model.index -> model_dir/0/model.index\n",
            "2025-02-14 11:40:32,544 INFO     [RVCModelSlotGenerator] RVC:: slotInfo.modelFile model.pth\n",
            "voice_changer/RVC/RVCModelSlotGenerator.py:57: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  cpt = torch.load(modelPath, map_location=\"cpu\")\n",
            "2025-02-14 11:40:32,618 INFO     [RVCModelSlotGenerator] Official Model(pyTorch) : v2\n",
            "voice_changer/common/SafetensorsUtils.py:52: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  data: dict = torch.load(pt_filename, map_location=\"cpu\")\n",
            "2025-02-14 11:40:33,011 INFO     [ModelSlot] SlotInfo::: RVCModelSlot(slotIndex=-1, voiceChangerType='RVC', name='model', description='', credit='', termsOfUseUrl='', iconFile='', speakers={0: 'target'}, modelFile='model.safetensors', modelFileOnnx='', indexFile='model.index', defaultTune=0, defaultFormantShift=0, defaultIndexRatio=0, defaultProtect=0.5, isONNX=False, modelType='pyTorchRVCv2', modelTypeOnnx='onnxRVC', samplingRate=40000, f0=True, embChannels=768, embOutputLayer=12, useFinalProj=False, deprecated=False, embedder='hubert_base', sampleId='', version='v2')\n",
            "2025-02-14 11:40:33,016 INFO     [VoiceChangerManager] params, LoadModelParams(voiceChangerType='RVC', slot=0, isSampleMode=False, sampleId=None, files=[LoadModelParamFile(name='model.pth', kind='rvcModel', dir=''), LoadModelParamFile(name='model.index', kind='rvcIndex', dir='')], params={})\n",
            "2025-02-14 11:40:48,907 INFO     [ModelSlot] SlotInfo::: RVCModelSlot(slotIndex=0, voiceChangerType='RVC', name='model', description='', credit='', termsOfUseUrl='', iconFile='model_dir/0/df5648eae752452b132c4ba333184da5.jpg', speakers={'0': 'target'}, modelFile='model.safetensors', modelFileOnnx='', indexFile='model.index', defaultTune=0, defaultFormantShift=0, defaultIndexRatio=0, defaultProtect=0.5, isONNX=False, modelType='pyTorchRVCv2', modelTypeOnnx='onnxRVC', samplingRate=40000, f0=True, embChannels=768, embOutputLayer=12, useFinalProj=False, deprecated=False, embedder='hubert_base', sampleId='', version='v2')\n",
            "2025-02-14 11:40:53,257 INFO     [VoiceChangerManager] update configuration modelSlotIndex: 0\n",
            "2025-02-14 11:40:53,258 INFO     [VoiceChangerManager] Model slot is changed -1 -> 0\n",
            "2025-02-14 11:40:53,258 INFO     [VoiceChangerManager] Loading RVC...\n",
            "2025-02-14 11:40:53,258 INFO     [RVCr2] Allocated audio buffer size: 9792\n",
            "2025-02-14 11:40:53,258 INFO     [RVCr2] Allocated convert buffer size: 18080\n",
            "2025-02-14 11:40:53,258 INFO     [RVCr2] Allocated pitchf buffer size: 114\n",
            "2025-02-14 11:40:53,258 INFO     [RVCr2] Initializing...\n",
            "torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "2025-02-14 11:40:54,062 INFO     [RVCInferencerv2] Compiling JIT model...\n",
            "2025-02-14 11:40:56,094 INFO     [EmbedderManager] Loading embedder hubert_base\n",
            "2025-02-14 11:40:56,094 INFO     [OnnxLoader] Quantizing model...\n",
            "2025-02-14 11:40:56,095 INFO     [shape_inference] Performing symbolic shape inference...\n",
            "2025-02-14 11:41:23,686 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/feature_projection/layer_norm/Add_1_output_0\" not specified\n",
            "2025-02-14 11:41:23,776 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layer_norm/Add_1_output_0\" not specified\n",
            "2025-02-14 11:41:23,936 INFO     [matmul] Ignore MatMul due to non constant B: /[/hubert/encoder/layers.0/attention/MatMul]\n",
            "2025-02-14 11:41:24,011 INFO     [matmul] Ignore MatMul due to non constant B: /[/hubert/encoder/layers.0/attention/MatMul_1]\n",
            "2025-02-14 11:41:24,012 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.0/attention/Reshape_7_output_0\" not specified\n",
            "2025-02-14 11:41:24,086 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.0/layer_norm/Add_1_output_0\" not specified\n",
            "2025-02-14 11:41:24,423 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.0/feed_forward/intermediate_act_fn/Mul_1_output_0\" not specified\n",
            "2025-02-14 11:41:24,561 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.0/final_layer_norm/Add_1_output_0\" not specified\n",
            "2025-02-14 11:41:24,718 INFO     [matmul] Ignore MatMul due to non constant B: /[/hubert/encoder/layers.1/attention/MatMul]\n",
            "2025-02-14 11:41:24,797 INFO     [matmul] Ignore MatMul due to non constant B: /[/hubert/encoder/layers.1/attention/MatMul_1]\n",
            "2025-02-14 11:41:24,798 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.1/attention/Reshape_7_output_0\" not specified\n",
            "2025-02-14 11:41:24,875 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.1/layer_norm/Add_1_output_0\" not specified\n",
            "2025-02-14 11:41:25,201 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.1/feed_forward/intermediate_act_fn/Mul_1_output_0\" not specified\n",
            "2025-02-14 11:41:25,318 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.1/final_layer_norm/Add_1_output_0\" not specified\n",
            "2025-02-14 11:41:25,480 INFO     [matmul] Ignore MatMul due to non constant B: /[/hubert/encoder/layers.2/attention/MatMul]\n",
            "2025-02-14 11:41:25,553 INFO     [matmul] Ignore MatMul due to non constant B: /[/hubert/encoder/layers.2/attention/MatMul_1]\n",
            "2025-02-14 11:41:25,554 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.2/attention/Reshape_7_output_0\" not specified\n",
            "2025-02-14 11:41:25,628 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.2/layer_norm/Add_1_output_0\" not specified\n",
            "2025-02-14 11:41:25,957 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.2/feed_forward/intermediate_act_fn/Mul_1_output_0\" not specified\n",
            "2025-02-14 11:41:26,071 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.2/final_layer_norm/Add_1_output_0\" not specified\n",
            "2025-02-14 11:41:26,228 INFO     [matmul] Ignore MatMul due to non constant B: /[/hubert/encoder/layers.3/attention/MatMul]\n",
            "2025-02-14 11:41:26,316 INFO     [matmul] Ignore MatMul due to non constant B: /[/hubert/encoder/layers.3/attention/MatMul_1]\n",
            "2025-02-14 11:41:26,318 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.3/attention/Reshape_7_output_0\" not specified\n",
            "2025-02-14 11:41:26,451 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.3/layer_norm/Add_1_output_0\" not specified\n",
            "2025-02-14 11:41:26,966 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.3/feed_forward/intermediate_act_fn/Mul_1_output_0\" not specified\n",
            "2025-02-14 11:41:27,120 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.3/final_layer_norm/Add_1_output_0\" not specified\n",
            "2025-02-14 11:41:27,354 INFO     [matmul] Ignore MatMul due to non constant B: /[/hubert/encoder/layers.4/attention/MatMul]\n",
            "2025-02-14 11:41:27,468 INFO     [matmul] Ignore MatMul due to non constant B: /[/hubert/encoder/layers.4/attention/MatMul_1]\n",
            "2025-02-14 11:41:27,469 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.4/attention/Reshape_7_output_0\" not specified\n",
            "2025-02-14 11:41:27,593 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.4/layer_norm/Add_1_output_0\" not specified\n",
            "2025-02-14 11:41:28,088 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.4/feed_forward/intermediate_act_fn/Mul_1_output_0\" not specified\n",
            "2025-02-14 11:41:28,241 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.4/final_layer_norm/Add_1_output_0\" not specified\n",
            "2025-02-14 11:41:28,464 INFO     [matmul] Ignore MatMul due to non constant B: /[/hubert/encoder/layers.5/attention/MatMul]\n",
            "2025-02-14 11:41:28,576 INFO     [matmul] Ignore MatMul due to non constant B: /[/hubert/encoder/layers.5/attention/MatMul_1]\n",
            "2025-02-14 11:41:28,578 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.5/attention/Reshape_7_output_0\" not specified\n",
            "2025-02-14 11:41:28,710 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.5/layer_norm/Add_1_output_0\" not specified\n",
            "2025-02-14 11:41:29,276 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.5/feed_forward/intermediate_act_fn/Mul_1_output_0\" not specified\n",
            "2025-02-14 11:41:29,398 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.5/final_layer_norm/Add_1_output_0\" not specified\n",
            "2025-02-14 11:41:29,561 INFO     [matmul] Ignore MatMul due to non constant B: /[/hubert/encoder/layers.6/attention/MatMul]\n",
            "2025-02-14 11:41:29,642 INFO     [matmul] Ignore MatMul due to non constant B: /[/hubert/encoder/layers.6/attention/MatMul_1]\n",
            "2025-02-14 11:41:29,643 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.6/attention/Reshape_7_output_0\" not specified\n",
            "2025-02-14 11:41:29,725 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.6/layer_norm/Add_1_output_0\" not specified\n",
            "2025-02-14 11:41:30,073 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.6/feed_forward/intermediate_act_fn/Mul_1_output_0\" not specified\n",
            "2025-02-14 11:41:30,203 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.6/final_layer_norm/Add_1_output_0\" not specified\n",
            "2025-02-14 11:41:30,367 INFO     [matmul] Ignore MatMul due to non constant B: /[/hubert/encoder/layers.7/attention/MatMul]\n",
            "2025-02-14 11:41:30,452 INFO     [matmul] Ignore MatMul due to non constant B: /[/hubert/encoder/layers.7/attention/MatMul_1]\n",
            "2025-02-14 11:41:30,453 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.7/attention/Reshape_7_output_0\" not specified\n",
            "2025-02-14 11:41:30,534 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.7/layer_norm/Add_1_output_0\" not specified\n",
            "2025-02-14 11:41:30,890 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.7/feed_forward/intermediate_act_fn/Mul_1_output_0\" not specified\n",
            "2025-02-14 11:41:31,028 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.7/final_layer_norm/Add_1_output_0\" not specified\n",
            "2025-02-14 11:41:31,188 INFO     [matmul] Ignore MatMul due to non constant B: /[/hubert/encoder/layers.8/attention/MatMul]\n",
            "2025-02-14 11:41:31,265 INFO     [matmul] Ignore MatMul due to non constant B: /[/hubert/encoder/layers.8/attention/MatMul_1]\n",
            "2025-02-14 11:41:31,266 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.8/attention/Reshape_7_output_0\" not specified\n",
            "2025-02-14 11:41:31,341 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.8/layer_norm/Add_1_output_0\" not specified\n",
            "2025-02-14 11:41:31,673 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.8/feed_forward/intermediate_act_fn/Mul_1_output_0\" not specified\n",
            "2025-02-14 11:41:31,790 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.8/final_layer_norm/Add_1_output_0\" not specified\n",
            "2025-02-14 11:41:31,947 INFO     [matmul] Ignore MatMul due to non constant B: /[/hubert/encoder/layers.9/attention/MatMul]\n",
            "2025-02-14 11:41:32,036 INFO     [matmul] Ignore MatMul due to non constant B: /[/hubert/encoder/layers.9/attention/MatMul_1]\n",
            "2025-02-14 11:41:32,038 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.9/attention/Reshape_7_output_0\" not specified\n",
            "2025-02-14 11:41:32,115 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.9/layer_norm/Add_1_output_0\" not specified\n",
            "2025-02-14 11:41:32,447 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.9/feed_forward/intermediate_act_fn/Mul_1_output_0\" not specified\n",
            "2025-02-14 11:41:32,562 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.9/final_layer_norm/Add_1_output_0\" not specified\n",
            "2025-02-14 11:41:32,714 INFO     [matmul] Ignore MatMul due to non constant B: /[/hubert/encoder/layers.10/attention/MatMul]\n",
            "2025-02-14 11:41:32,791 INFO     [matmul] Ignore MatMul due to non constant B: /[/hubert/encoder/layers.10/attention/MatMul_1]\n",
            "2025-02-14 11:41:32,792 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.10/attention/Reshape_7_output_0\" not specified\n",
            "2025-02-14 11:41:32,869 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.10/layer_norm/Add_1_output_0\" not specified\n",
            "2025-02-14 11:41:33,209 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.10/feed_forward/intermediate_act_fn/Mul_1_output_0\" not specified\n",
            "2025-02-14 11:41:33,328 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.10/final_layer_norm/Add_1_output_0\" not specified\n",
            "2025-02-14 11:41:33,485 INFO     [matmul] Ignore MatMul due to non constant B: /[/hubert/encoder/layers.11/attention/MatMul]\n",
            "2025-02-14 11:41:33,565 INFO     [matmul] Ignore MatMul due to non constant B: /[/hubert/encoder/layers.11/attention/MatMul_1]\n",
            "2025-02-14 11:41:33,567 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.11/attention/Reshape_7_output_0\" not specified\n",
            "2025-02-14 11:41:33,648 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.11/layer_norm/Add_1_output_0\" not specified\n",
            "2025-02-14 11:41:33,962 INFO     [onnx_quantizer] Quantization parameters for tensor:\"/hubert/encoder/layers.11/feed_forward/intermediate_act_fn/Mul_1_output_0\" not specified\n",
            "2025-02-14 11:41:34,942 INFO     [OnnxLoader] Done!\n",
            "2025-02-14 11:41:35,526 INFO     [PitchExtractorManager] Loading pitch extractor rmvpe_onnx\n",
            "2025-02-14 11:41:38,980 INFO     [PipelineGenerator] Loading index...\n",
            "2025-02-14 11:41:38,980 INFO     [PipelineGenerator] Try loading \"model_dir/0/model.index\"...\n",
            "faiss/contrib/torch_utils.py:51: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "2025-02-14 11:41:39,039 INFO     [Pipeline] GENERATE INFERENCER<voice_changer.RVC.inferencer.RVCInferencerv2.RVCInferencerv2 object at 0x7fa130d38ac0>\n",
            "2025-02-14 11:41:39,039 INFO     [Pipeline] GENERATE EMBEDDER<voice_changer.embedder.OnnxContentvec.OnnxContentvec object at 0x7fa0fc06f700>\n",
            "2025-02-14 11:41:39,039 INFO     [Pipeline] GENERATE PITCH EXTRACTOR<voice_changer.pitch_extractor.RMVPEOnnxPitchExtractor.RMVPEOnnxPitchExtractor object at 0x7fa10a90b8e0>\n",
            "2025-02-14 11:41:39,103 INFO     [RVCr2] Initialized.\n",
            "2025-02-14 11:42:02,101 INFO     [VoiceChangerManager] update configuration gpu: 0\n",
            "2025-02-14 11:42:02,102 INFO     [DeviceManager] Switched to 0: Tesla T4 (CUDA) (cuda:0). FP16 support: True\n",
            "2025-02-14 11:42:02,391 INFO     [VoiceChangerV2] Allocated SOLA buffer size: 4800\n",
            "2025-02-14 11:42:02,391 INFO     [RVCr2] Initializing...\n",
            "2025-02-14 11:42:03,282 INFO     [RVCInferencerv2] Compiling JIT model...\n",
            "2025-02-14 11:42:05,954 INFO     [EmbedderManager] Loading embedder hubert_base\n",
            "2025-02-14 11:42:05,954 INFO     [OnnxLoader] Converting model to FP16...\n",
            "2025-02-14 11:42:14,685 INFO     [OnnxLoader] Done!\n",
            "2025-02-14 11:42:16,618 INFO     [PitchExtractorManager] Loading pitch extractor rmvpe_onnx\n",
            "2025-02-14 11:42:16,619 INFO     [OnnxLoader] Converting model to FP16...\n",
            "2025-02-14 11:42:26,309 WARNING  [symbolic_shape_infer] Unable to determine if n_samples <= ConvTranspose_308_o0__d2, treat as equal\n",
            "2025-02-14 11:42:27,185 INFO     [OnnxLoader] Done!\n",
            "\u001b[0;93m2025-02-14 11:42:29.139978229 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 2 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n",
            "2025-02-14 11:42:29,302 INFO     [PipelineGenerator] Loading index...\n",
            "2025-02-14 11:42:29,302 INFO     [PipelineGenerator] Try loading \"model_dir/0/model.index\"...\n",
            "2025-02-14 11:42:30,595 INFO     [Pipeline] GENERATE INFERENCER<voice_changer.RVC.inferencer.RVCInferencerv2.RVCInferencerv2 object at 0x7fa130c77d30>\n",
            "2025-02-14 11:42:30,595 INFO     [Pipeline] GENERATE EMBEDDER<voice_changer.embedder.OnnxContentvec.OnnxContentvec object at 0x7fa0fc0d6ec0>\n",
            "2025-02-14 11:42:30,595 INFO     [Pipeline] GENERATE PITCH EXTRACTOR<voice_changer.pitch_extractor.RMVPEOnnxPitchExtractor.RMVPEOnnxPitchExtractor object at 0x7fa0f17385e0>\n",
            "2025-02-14 11:42:30,669 INFO     [RVCr2] Initialized.\n",
            "2025-02-14 11:42:30,673 INFO     [RVCr2] Allocated audio buffer size: 9792\n",
            "2025-02-14 11:42:30,674 INFO     [RVCr2] Allocated convert buffer size: 18080\n",
            "2025-02-14 11:42:30,674 INFO     [RVCr2] Allocated pitchf buffer size: 114\n"
          ]
        }
      ],
      "source": [
        "#=======================Updated=========================\n",
        "import codecs\n",
        "import json\n",
        "import subprocess, threading, time, socket, urllib.request, portpicker, json\n",
        "from IPython.display import clear_output, Javascript\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "# @title **Cell[2]** Start Server using **NGROK** or **HRZN**\n",
        "# @markdown This cell will start the server, the first time that you run it will download the models, so it can take a while (~1-2 minutes)\n",
        "\n",
        "#======================Tunnels===========================\n",
        "\n",
        "TUNNEL = \"NGROK\" #@param [\"NGROK\",\"HRZN\"]\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown You'll need a NGROK or HRZN account, but <font color=green>**it's free**</font> and easy to create!\n",
        "# @markdown ---\n",
        "# @markdown **1** - Create a <font color=green>**free**</font> account at [ngrok](https://dashboard.ngrok.com/signup) / [hrzn](https://hrzn.run/login) or **login with Google/Github account**\\\n",
        "# @markdown **2** - If you didn't logged in with Google/Github, you will need to **verify your e-mail**!\\\n",
        "# @markdown **3** - Get your [ngrok](https://dashboard.ngrok.com/get-started/your-authtoken) or [hrzn](https://hrzn.run/dashboard) to get your auth token, and place it here:\n",
        "Token = '2ihZnAI1MCjOMLpXckwJVhPojpt_7gBro3cjDSPFMneQ5v6H4' # @param {type:\"string\"}\n",
        "# @markdown **4** - *(OPTIONAL FOR NGROK)* Change to a region near to you\\\n",
        "# @markdown `Default Region: ap - Asia/Pacific (Singapore)`\n",
        "Region = \"ap - Asia/Pacific (Singapore)\" # @param [\"ap - Asia/Pacific (Singapore)\", \"au - Australia (Sydney)\",\"eu - Europe (Frankfurt)\", \"in - India (Mumbai)\",\"jp - Japan (Tokyo)\",\"sa - South America (Sao Paulo)\", \"us - United States (Ohio)\"]\n",
        "\n",
        "#@markdown **5** - *(optional)* Other options:\n",
        "ClearConsole = True  # @param {type:\"boolean\"}\n",
        "\n",
        "# ---------------------------------\n",
        "# DO NOT TOUCH ANYTHING DOWN BELOW!\n",
        "# ---------------------------------\n",
        "if version == \"V1(new)\":\n",
        "\n",
        "    PORT = 18888\n",
        "    if not globals().get('Ready', False):\n",
        "        print(\"Go back and run first cells.\")\n",
        "    else:\n",
        "        if TUNNEL == \"NGROK\":\n",
        "            if not globals().get('Ready', False):\n",
        "                print(\"Go back and run first and second cells.\")\n",
        "            else:\n",
        "                from pyngrok import conf, ngrok\n",
        "                MyConfig = conf.PyngrokConfig()\n",
        "                MyConfig.auth_token = Token\n",
        "                MyConfig.region = Region[0:2]\n",
        "                conf.get_default().authtoken = Token\n",
        "                conf.get_default().region = Region\n",
        "                conf.set_default(MyConfig)\n",
        "\n",
        "                import threading, time, socket\n",
        "                import json\n",
        "                from pyngrok import ngrok\n",
        "                from IPython.display import clear_output\n",
        "\n",
        "                ngrokConnection = ngrok.connect(PORT)\n",
        "                public_url = ngrokConnection.public_url\n",
        "\n",
        "        elif TUNNEL == \"HRZN\":\n",
        "            !rm -rf url.txt\n",
        "            !hrzn login $Token\n",
        "            os.system(f\"hrzn tunnel http://localhost:{PORT} >> url.txt 2>&1 &\")\n",
        "            time.sleep(5)\n",
        "\n",
        "            with open('url.txt', 'r') as file:\n",
        "                public_url = file.read()\n",
        "                public_url = !grep -oE \"https://[a-zA-Z0-9.-]+\\.hrzn\\.run\" url.txt\n",
        "                public_url = public_url[0]\n",
        "\n",
        "        set_key('.env', \"ALLOWED_ORIGINS\", json.dumps([public_url]))\n",
        "\n",
        "        def wait_for_server():\n",
        "            while True:\n",
        "                time.sleep(0.5)\n",
        "                sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "                result = sock.connect_ex(('127.0.0.1', PORT))\n",
        "                if result == 0:\n",
        "                    break\n",
        "                sock.close()\n",
        "            if ClearConsole:\n",
        "                clear_output()\n",
        "            print(\"--------- SERVER READY! ---------\")\n",
        "            print(\"Your server is available at:\")\n",
        "            print(public_url)\n",
        "            print(\"---------------------------------\")\n",
        "\n",
        "        threading.Thread(target=wait_for_server, daemon=True).start()\n",
        "\n",
        "        !./$path\n",
        "\n",
        "        clear_output()\n",
        "        if TUNNEL == \"NGROK\":\n",
        "            ngrok.disconnect(ngrokConnection.public_url)\n",
        "            print(\"--------- SERVER STOPPED! ---------\")\n",
        "        elif TUNNEL == \"HRZN\":\n",
        "            !rm -rf url.txt\n",
        "            !fuser -k ${PORT}\n",
        "            print(\"--------- SERVER STOPPED! ---------\")\n",
        "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
        "\n",
        "elif version == \"V1(ori)\":\n",
        "\n",
        "    # Check if Run_Cell\n",
        "    PORT = portpicker.pick_unused_port()\n",
        "    if 'Run_Cell' not in globals():\n",
        "        print(\"No, Go back to the first cell and run it\")\n",
        "    else:\n",
        "        if Run_Cell == 0:\n",
        "            print(\"No, Go back to the first cell and run it\")\n",
        "        else:\n",
        "            if TUNNEL == \"NGROK\":\n",
        "              from pyngrok import conf, ngrok\n",
        "              MyConfig = conf.PyngrokConfig()\n",
        "              MyConfig.auth_token = Token\n",
        "              MyConfig.region = Region[0:2]\n",
        "              conf.set_default(MyConfig)\n",
        "              ngrokConnection = ngrok.connect(PORT)\n",
        "              public_url = ngrokConnection.public_url\n",
        "            elif TUNNEL == \"HRZN\":\n",
        "              !rm -rf url.txt\n",
        "              !hrzn login $Token\n",
        "              os.system(f\"hrzn tunnel http://localhost:{PORT} >> url.txt 2>&1 &\")\n",
        "              time.sleep(5)\n",
        "\n",
        "              with open('url.txt', 'r') as file:\n",
        "                public_url = file.read()\n",
        "                public_url = !grep -oE \"https://[a-zA-Z0-9.-]+\\.hrzn\\.run\" url.txt\n",
        "                public_url = public_url[0]\n",
        "\n",
        "            def wait_for_server():\n",
        "                while True:\n",
        "                    time.sleep(0.5)\n",
        "                    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "                    result = sock.connect_ex(('127.0.0.1', PORT))\n",
        "                    if result == 0:\n",
        "                        break\n",
        "                    sock.close()\n",
        "                if ClearConsole:\n",
        "                    clear_output()\n",
        "                print(\"--------- SERVER READY! ---------\")\n",
        "                print(\"Your server is available at:\")\n",
        "                print(public_url)\n",
        "                print(\"---------------------------------\")\n",
        "\n",
        "            threading.Thread(target=wait_for_server, daemon=True).start()\n",
        "\n",
        "            mainpy = codecs.decode('ZZIPFreireFVB.cl', 'rot_13')\n",
        "            mainname = codecs.decode('ZZIPFreireFVB', 'rot_13')\n",
        "            !mv {mainpy} HVoice.py\n",
        "            !sed -i \"s/MMVCServerSIO/HVoice/\" HVoice.py\n",
        "            !python3 HVoice.py \\\n",
        "              -p {PORT} \\\n",
        "              --https False \\\n",
        "              --content_vec_500 pretrain/checkpoint_best_legacy_500.pt \\\n",
        "              --content_vec_500_onnx pretrain/content_vec_500.onnx \\\n",
        "              --content_vec_500_onnx_on false \\\n",
        "              --hubert_base pretrain/hubert_base.pt \\\n",
        "              --hubert_base_jp pretrain/rinna_hubert_base_jp.pt \\\n",
        "              --hubert_soft pretrain/hubert/hubert-soft-0d54a1f4.pt \\\n",
        "              --nsf_hifigan pretrain/nsf_hifigan/model \\\n",
        "              --crepe_onnx_full pretrain/crepe_onnx_full.onnx \\\n",
        "              --crepe_onnx_tiny pretrain/crepe_onnx_tiny.onnx \\\n",
        "              --rmvpe pretrain/rmvpe.pt \\\n",
        "              --model_dir model_dir \\\n",
        "              --samples samples.json \\\n",
        "              --allowed-origins {public_url}\n",
        "\n",
        "            clear_output()\n",
        "            if TUNNEL == \"NGROK\":\n",
        "              ngrok.disconnect(ngrokConnection.public_url)\n",
        "              print(\"--------- SERVER STOPPED! ---------\")\n",
        "            elif TUNNEL == \"HRZN\":\n",
        "              !rm -rf url.txt\n",
        "              !fuser -k ${PORT}\n",
        "              print(\"--------- SERVER STOPPED! ---------\")\n",
        "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
        "\n",
        "elif version == \"V2\":\n",
        "\n",
        "    import time\n",
        "    from IPython.display import clear_output, display, Javascript\n",
        "    PORT = portpicker.pick_unused_port()\n",
        "    LOG_FILE = f\"/content/LOG_FILE_{PORT}\"\n",
        "\n",
        "    # Start\n",
        "    if mode == \"elf\":\n",
        "        get_ipython().system_raw(f'LD_LIBRARY_PATH=/usr/lib64-nvidia:/usr/lib/x86_64-linux-gnu ./vcclient_latest_for_colab cui --port {PORT} --no_cui true >{LOG_FILE} 2>&1 &')\n",
        "    elif mode == \"zip\":\n",
        "        !LD_LIBRARY_PATH=/usr/lib64-nvidia:/usr/lib/x86_64-linux-gnu ./main cui --port {PORT} --no_cui true &\n",
        "\n",
        "    # Tunggu sampai server dimulai\n",
        "    print('\\033[31m\\033[1m\\033[3mTunggu sampai server dimulai\\033[0m')\n",
        "    time.sleep(130)\n",
        "\n",
        "    if TUNNEL == \"NGROK\":\n",
        "\n",
        "        from pyngrok import ngrok\n",
        "        Close_Ngrok = True\n",
        "        Open_New_Tab = True\n",
        "\n",
        "        from pyngrok import conf, ngrok\n",
        "        MyConfig = conf.PyngrokConfig()\n",
        "        MyConfig.auth_token = Token\n",
        "        MyConfig.region = Region[0:2]\n",
        "        conf.set_default(MyConfig)\n",
        "        ngrokConnection = ngrok.connect(PORT)\n",
        "        public_url = ngrokConnection.public_url\n",
        "        clear_output()\n",
        "\n",
        "        if Open_New_Tab:\n",
        "            display(Javascript(f'window.open(\"{public_url}\", \"_blank\");'))\n",
        "\n",
        "        print(\"--------- SERVER LINK ---------\")\n",
        "        print('\\033[32m\\033[1m\\033[3mServer Sudah Berjalan\\033[0m')\n",
        "        print(\"PUBLIC URL:\", public_url)\n",
        "        print(\"---------------------------------\")\n",
        "\n",
        "    elif TUNNEL == \"HRZN\":\n",
        "        !rm -rf url.txt\n",
        "        !hrzn login $Token\n",
        "        os.system(f\"hrzn tunnel http://localhost:{PORT} >> url.txt 2>&1 &\")\n",
        "        time.sleep(5)\n",
        "\n",
        "        with open('url.txt', 'r') as file:\n",
        "          public_url = file.read()\n",
        "          public_url = !grep -oE \"https://[a-zA-Z0-9.-]+\\.hrzn\\.run\" url.txt\n",
        "          public_url = public_url[0]\n",
        "\n",
        "        print(\"--------- SERVER LINK ---------\")\n",
        "        print('\\033[32m\\033[1m\\033[3mServer Sudah Berjalan\\033[0m')\n",
        "        print(\"PUBLIC URL:\", public_url)\n",
        "        print(\"---------------------------------\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import requests\n",
        "import codecs\n",
        "\n",
        "if version == \"V1(ori)\":\n",
        "\n",
        "    #@title **[Optional Cell For V1 Ori Okada Voice Changer]** Upload a voice model (Run this before running the Voice Changer)\n",
        "    #@markdown Find your model here [voice-models](https://voice-models.com/)\n",
        "    # @markdown ---\n",
        "    model_slot = \"4\" #@param ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199']\n",
        "\n",
        "    !rm -rf model_dir/$model_slot\n",
        "    #@markdown **[Optional]** Add an icon to the model (Kosongin aja gapapa / It's okay to leave it blank)\n",
        "    icon_link = \"\"  #@param {type:\"string\"}\n",
        "    icon_link = '\"'+icon_link+'\"'\n",
        "    !mkdir model_dir\n",
        "    !mkdir model_dir/$model_slot\n",
        "    #@markdown Put your model's download link here `(must be a zip file and don't use GPT-SoVITS Model)` only supports **huggingface.co** & **google drive**<br>\n",
        "    model_link = \"https://huggingface.co/RegalHyperus/new-rvc-models/resolve/main/IbukiTangaJP.zip\"  #@param {type:\"string\"}\n",
        "\n",
        "    if model_link.startswith(\"https://www.weights.gg\") or model_link.startswith(\"https://weights.gg\"):\n",
        "        print(\"Links from weights.gg is no longer supported.\")\n",
        "        sys.exit()\n",
        "    elif model_link.startswith(\"https://drive.google.com\"):\n",
        "        model_link = '\"'+model_link+'\"'\n",
        "        !gdown $model_link --fuzzy -O model.zip\n",
        "        print(\"Model from Drive\")\n",
        "    elif model_link.startswith(\"https://huggingface.co\"):\n",
        "        model_link = model_link\n",
        "        model_link = '\"'+model_link+'\"'\n",
        "        !curl -L $model_link > model.zip\n",
        "        print(\"Model from huggingface Link\")\n",
        "    else:\n",
        "        model_link = model_link\n",
        "        model_link = '\"'+model_link+'\"'\n",
        "        !curl -L -O $model_link\n",
        "        !mv ./*.pth model_dir/$model_slot/\n",
        "        print('Model(.pth) or a direct model link.')\n",
        "\n",
        "    # Conditionally set the iconFile based on whether icon_link is empty\n",
        "    if icon_link == '\"\"':\n",
        "        iconFile = \"\"\n",
        "        print(\"icon_link is empty, so no icon file will be downloaded.\")\n",
        "    else:\n",
        "        iconFile = \"icon.png\"\n",
        "        !curl -L $icon_link > model_dir/$model_slot/icon.png\n",
        "\n",
        "    !unzip model.zip -d model_dir/$model_slot\n",
        "\n",
        "    !mv model_dir/$model_slot/*/* model_dir/$model_slot/\n",
        "    !rm -rf model_dir/$model_slot/*/\n",
        "    !rm -rf model.zip\n",
        "    #@markdown **Model Voice Conversion Setting**\n",
        "    Tune = 12  #@param {type:\"slider\",min:-24,max:24,step:1}\n",
        "    Index = 0  #@param {type:\"slider\",min:0,max:1,step:0.1}\n",
        "\n",
        "    param_link = \"\"\n",
        "    if param_link == \"\":\n",
        "        paramset = requests.get(\"https://pastebin.com/raw/DuznQ4Fg\").text\n",
        "        exec(paramset)\n",
        "\n",
        "    clear_output()\n",
        "    print(\"\\033[93mModel with the name of \"+model_name+\" has been Imported to slot \"+model_slot)\n",
        "\n",
        "elif version == \"V1(new)\":\n",
        "    print('\\033[31m\\033[1m\\033[3mV1 New Belum Support / V1 New is not supported\\033[0m')\n",
        "elif version == \"V2\":\n",
        "    print('\\033[31m\\033[1m\\033[3mV2 Belum Support / V2 is not supported\\033[0m')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "N2c7UpbA3eYr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}